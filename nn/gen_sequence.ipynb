{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified ./notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified ./notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for ./notMNIST_large. This may take a while. Please wait.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "Extracting data for ./notMNIST_small. This may take a while. Please wait.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force: #See if root is an existing directory and if force = False\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABwUlEQVR4nG2SzYuNYRjGr+d53nN8\nHbOQcUjTkRUlcrKysJSyYwxpQo2FnfI3nCIL69mMhTqLqdnIYighC6RpIpNSSJQmHzUlnc77Pvfz\nszjvcD5c2+u+ur9+Tus6fKWZVuaeOTQit+kjpGRn5Uc8rxZmqcvjutxwTru/k4CcIyPRim4DQKS9\neSjqdeALCYCCqSGzojmMUj8aA15Q8wMGpATwaCBa1Q0i/HVbyvo6TjzvmQt3yIHOtKohhDJ4gZRI\nrJ0YXyKSWD2qUO6Y6SZdMJakxlMM4+3pyakzNWXCjp9UkJx0qpF35OW1f0ExOzcvr0ny3hJpfRss\nxshFealbDpaIsYhmJnnC/KG2PHrxoBIl4X2WVbIQApJU/Ryd8+ngw/HkJb7leZF3vr6OLZzy6nTb\nS7rW6/ly5/ZaJkluFsNYrkvatdK761U5yYWQqb6WEjkzkprlsHvlXHnUy3SjFb/r0nkKSLyp/Pv8\n2AKF8aSmbJYIxvV+LrbM3Hu/2JDG3pEgcmwQG20IkiYwSPzc0f9FF5yck98ok5Lu/1IfsBgO5Fdf\nVcxMi50h5nql+5aJfNrzH5glp62X7t7aNoKyJP0BHnoqWJECwiIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACu0lEQVR4nH2TS0hUYQCFz73/74xj\no443M+dhJk0zNvagQqw2EghmIIGVLaKwInpQVJIWVAQ97LUohFpky+iJRkaQ0KJCS3MRLZycUcvG\n1BidnPF1vd7/0aKgVp7N2X2cxXcgOROcccmY5EyIv2XwFhCovCwQrAtsTqwPRKoDzc8Cq0Z3Bdru\nLb/iIIRQdCQme8JRsxN6V3iIhWF8Co8N9PSWcA5FBI382IjV38ULhsYKZFBZ/m3Sqw9mZXwRcSqf\nR092vss6/mTq7OvPhkxGXUvfocHm4sL72adhAq3boESBYAkAYMiNxhpUtgIzlDT88h4s1FLrp50p\nlAipiusDazyZa73XNAuFb9rmMueRwIzFZFJIIpY47HKly7YyBWCZ6NiDBSNAaKtCVUX9uQIvzmNf\nBzKZilQQG1JVO1RdMiGFmAdqhRWwc0VGZnMm4hZ3hOeOTs0gGXk/ddfsaFZab7JH4dXDV9+8cl08\nNln/8EO1uEVv3+w+0/dIGDahgxG0bQeNAsENuHkDGFiEphMAAFC1cTy/pjwt7bHhudpfKheS+Xej\nRd7Vhu7gcYp/MflTaecAoGLdsgeZVf9jNwIABhehqRaVb6HqVDky7CxTXLa9E5qDEiFVVH1ZQirL\nciqdFkUOzHomEhZ3xFxc3kKEJF/ptNuIOew/LDlgTnzcD9coRbgCRIEyvAYvL+DARzgZhcewappH\nzR2j4s8up2ZL1zSr5gFkPGZOxeLiV5SXgigg/YmRWT02ZcbiEqzU33XJvyle5P9eQa1JNGlop7+1\nwX+5y1/KKN5PjHeHhs0O6AnGAMjOcKw/FBoPDTKq1EbdWzKyUk5Nz9+Rb8hk2A/3+VLGi91Hs61z\nCgaWjvbdcIwA3SW4Uw/8WIbmc6hqB0BR1G9f6stLKkzYCiIu7kuyruYZOUu9CiEcc93hN211a7Sq\n1ncVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAUUlEQVR4nGP8z4AbMOGRY2CB0n+w\nSTLiMxaq88/m34xYZP//////3//PPAxYJFngNAumBQgH/cGUxOuVUUnaSZIfZdA0hD2ZEJPAyEia\neL0CAJenG37nLvsEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABWklEQVR4nI2RzStEURjGn/fcexmR\niT02SllMorC3UtbKnqV/wsJS2SqUyZSUJRtZS2xothosbDRKmYlzzvtY3A93xiVPnTrn/b1fPQf4\nKcHE8fN7Y7tcyEbq9FSeFkCDGbZIdTQFFEAASPAbjPuH2aD4AJSMxtCIEmAcIkwOGiowUAqTWPtV\nMyiipbmlyvhwfwAA0JeL67RW0LN8Q2WRgPIxvVr36bzGysGBc1rXkd+6qtGTJMP1BRcClJO9B58s\n1JhecT0AgHsl6bgV5Oyb5UdcOSoAfHDoI59CTW9hkC7tFd1KzWCRuX8Z/z8o+aB0wY6Z6SO0UZIc\nZPnp2hZ1knSs9X0Xlqp0JHknazsuEFBuj26bAIChyvIUBfThKqJ9WqeJ04k81VnuRkDvZpvqnbUu\nkbXOK1sbvYAYTFWbXZ+tzYMKDARilGOL85Mjg30CgO23p/rl2aMY5Rdau/xnnt2KUwAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nJ2SzQ3CMAyF82OuDNYV\nuLBRV+kWsAcTQKUKkaSPAyK1HdVV61OkT89+z46HW69gMEf1Vdoe3mr7V8LfH2HWFL9K6KyZZ6K8\nCmUVCPjKWljdSkNz6G+xVEO6Lo7EEliQTG9pKMYFnlyUcEpc+eGGMl0HnnNKQjk+m7ALJM+VkBCA\nvpB57OOQLYGaCx24ioIbM/d+TTPKF3S1V9V6j3LPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAfUlEQVR4nNWSOw6AIBBEAaHzYh7U\nC3kVEguRHQtZRPkE7dwGlpfZzwQJUQ/VYG2ow7mXoGz1DEo3b7JAAYBgR1GA3NNIkzdg6ODqUAmV\nQLpDEpQrwyp+2eNA0HZaz3dkcc3OZX2ynOaky9shUcYkwMdAxNcO+96b8P2b/Ase5I1JcMJ7nLQA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACbElEQVR4nFWRW0jTYRjGn+/7f3Nu\nOufcRPHQ8CInipZKUlLsIggSKYIORgVRBgkVhYQEBd14ERJ0oXTRCSsiwxsrC9QOSGpJlrgsTafo\nbFbOQ67Ztv/h7WLp1nP7e3ne531eBgBgXEXB3lJzaMaV8bNzEBAq4Z84x4bmufCKZ/TrdIBW3lYb\nkGIFAwMgqevuFF926exFBqH88gUtdmfLvqtNQhEAVzPLpj5svGQDKMj1wNRoX41pHgAH1xxDOY0v\n9LaR2tKs9AzH7huWHf7P8IIABuunhotuogYdGAAwpDSSQuWQAI671NUyTlcgCc4Z45LEscdPhyEA\nOEPnKk/RsAF8LbwOBzwl4ACuzbQ/7KezEIiK8zgAEPmHegdm7XgGLQZqXGYEiFoSW02bx9wgxFJG\nAIQjXr/AMamyWBgpB0Kt6bGlb/+N/yBFfETO+Z2KFf7I6GoefZLwK0wR84HpkAH6WKRteWSw+H2m\nlyK8fsaUiOyYsIT3oaT7mU65iXckAst/cs205ktS6E2i38dd3bxTGXjd2p+6KVoQGBaHZicmZYh3\n8xe6jToc7Yo9UzakOXOWwENtCSsBpu4vUqUoNKbKP8DAUe+tCLY/1tUjuhX5TwI6TIFLvipRUmhZ\nqKzThMQAJsUpeSVpxXnoACBQHb5ee9JHxyOZGArGbh/0kCsBACScIfnL096lusJkwFjeMDE4opE3\nH5wBEMqxWwMPjPptTuV70Ja82BauyHp+2s21SPvKruZkyK1zwmPPdVjjNX7zBGOrnUnIvhem8Leh\nnmmZaPlVFeMcWH2GpFHZkVIzk33e8eGPbnAiAH8BUgD2I3ucbkMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABmUlEQVR4nGXSP2/TQBzG8e/9fLGb\nf1QBhCohMUCrCqoOLIyUiQEh1IEJeDW8BhiY2JkQEgsrCxsMLChFEQhFQFRqO3exz3FshrhQ27d+\n9DzPSXdK/DynfZ7cNW/fEbx4+bDXxtdh+WFbXP/w2XOvheeDwv0SPn6/dOdev04yGG3k87nw7YTh\n4aCOnasBSYQwDdXg1kYTfdIYYRrh7wzqq51rPs4gzAxK39is406AswjWZKjtc+2kRSAOoatr6O8G\nZBYBO4dex/NEREQpQA+3OmQJGuYxeOXqf071tgTyBA12jrp5/3qWOZem9sStynhSeFXSGtTBQZUq\ns+Ppl89x4onoCll9mnkiIp7u9q/sPQbwu1Ut5umb00Gluhd2Hz0Y9S+iYWEg+femZYkxM3V7NLpc\n1Zb27IMrVmMHcppcnMUSN4nyPEUgsWAbX6WcRNpH1rc1yzpyFOoAgTxN67UA49DzESCLymYtX6sk\nLmptMg7Xm7iwME38fbxcY7GUbNVAfsx8NPDz/fDINnH6J0ABeJuLrGjg/n7+6i9+KrLmr7qVDQAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAFklEQVR4nGP8z4AbMOGRG5UclRxh\nkgCD/gE3CChK/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABjUlEQVR4nHVRPUhcQRicb3ffPVAS\nyOmJ0cZGk9JaG9OlsouNYCmCXRIsAhGE/BAsUmhhJf6AhFhYWRqLE0Sw0E4wlQiBFEHQSy6+b3dS\n3Lt373lmqmVnZ77Z+YAmIszU9U+t/mMELr0yaCEu2bjD9r4tq9xDCgCXPJ8B75KCuGEf3o/eYxuB\nAsBgtbOdLKV2HFqCbZuZDZic8u4/SoTSwpDaYqAoS+kHFiXInUBNmDD+irZAuuwkwJtRdS2SjYSp\nVB+9q2heWXBJxmYLE78yYQuez/KvBXkEqBMRoW8nk2j50DH9m7BAanTySV2lZ2D410ZNKM0KACC4\n2stLi6pX7lcgiLCdBQrKOTiYqlFEDoDAZ0JvvywaDwzeJvz5BAYxNptKZfUBDGDOP7jflWEQQD3T\nnby4NgEAHu7Qf++CxFhrKJWnfWlbBuVdch7SgS0mJJVH/VmTBuUVz1mg/5jK8Jd73bkViMP4Bb9N\nrFO9Bm6VCxsQQffHawYy8GpabLFjWMHj1wdXN2efnyLH/QMl8rT87w7uTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for folder in train_folders:\n",
    "    display(Image(filename = os.path.join(folder,os.listdir(folder)[np.random.randint(0,100)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling ./notMNIST_large/A.pickle.\n",
      "./notMNIST_large/A\n",
      "Could not read: ./notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file './notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: ./notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file './notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: ./notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file './notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n",
      "Pickling ./notMNIST_large/B.pickle.\n",
      "./notMNIST_large/B\n",
      "Could not read: ./notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file './notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00756303\n",
      "Standard deviation: 0.454491\n",
      "Pickling ./notMNIST_large/C.pickle.\n",
      "./notMNIST_large/C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling ./notMNIST_large/D.pickle.\n",
      "./notMNIST_large/D\n",
      "Could not read: ./notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file './notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling ./notMNIST_large/E.pickle.\n",
      "./notMNIST_large/E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling ./notMNIST_large/F.pickle.\n",
      "./notMNIST_large/F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling ./notMNIST_large/G.pickle.\n",
      "./notMNIST_large/G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling ./notMNIST_large/H.pickle.\n",
      "./notMNIST_large/H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling ./notMNIST_large/I.pickle.\n",
      "./notMNIST_large/I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling ./notMNIST_large/J.pickle.\n",
      "./notMNIST_large/J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling ./notMNIST_small/A.pickle.\n",
      "./notMNIST_small/A\n",
      "Could not read: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file './notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling ./notMNIST_small/B.pickle.\n",
      "./notMNIST_small/B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      "Pickling ./notMNIST_small/C.pickle.\n",
      "./notMNIST_small/C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling ./notMNIST_small/D.pickle.\n",
      "./notMNIST_small/D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling ./notMNIST_small/E.pickle.\n",
      "./notMNIST_small/E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling ./notMNIST_small/F.pickle.\n",
      "./notMNIST_small/F\n",
      "Could not read: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file './notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling ./notMNIST_small/G.pickle.\n",
      "./notMNIST_small/G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling ./notMNIST_small/H.pickle.\n",
      "./notMNIST_small/H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling ./notMNIST_small/I.pickle.\n",
      "./notMNIST_small/I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      "Pickling ./notMNIST_small/J.pickle.\n",
      "./notMNIST_small/J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./notMNIST_large/A.pickle',\n",
       " './notMNIST_large/B.pickle',\n",
       " './notMNIST_large/C.pickle',\n",
       " './notMNIST_large/D.pickle',\n",
       " './notMNIST_large/E.pickle',\n",
       " './notMNIST_large/F.pickle',\n",
       " './notMNIST_large/G.pickle',\n",
       " './notMNIST_large/H.pickle',\n",
       " './notMNIST_large/I.pickle',\n",
       " './notMNIST_large/J.pickle']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10868b810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "pickle_file = train_datasets[0]  # index 0 should be all As, 1 = all Bs, etc.\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set = pickle.load(f)  # unpickle\n",
    "    sample_idx = np.random.randint(len(letter_set))  # pick a random image index\n",
    "    sample_image = letter_set[sample_idx, :, :]  # extract a 2D slice\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image, cmap = cm.Greys_r)  # display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Letter = J \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = G \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = I \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = E \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = D \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = J \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = D \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = F \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = C \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Letter = G \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEH1JREFUeJzt3X2MXOV1x/Hf2fX6FUNtCI4xBoNkXF6Sgro1SCEVlEB4\niYRTRQirat0WxUTQKFFTtYhKLa3UQpsCRWqJamoXkyaQSgkvUlwQWE0JaUJYKC/mJdghBuwY28Qh\n4ADr3Z3TP/ZCF9h77nhmdu7Y5/uRrJ2dM3fu8fX+PLPz3Ps85u4CkE9f3Q0AqAfhB5Ii/EBShB9I\nivADSRF+ICnCDyRF+IGkCD+Q1LRu7my6zfCZmtPNXaJNvnR6WF8267WwPqZGaa0/8WtPO8dl8/Bh\npbW3Xnld+157y5rpoa3wm9n5km6S1C/pX939uujxMzVHp9s57ewS+6uvP643xsLy8M1Lwvp3Trkr\nrP+i8VZp7bC+WeG2B7O9jbdLa4f0zQy3/dTzF5TWvrf6G0330PJ/vWbWL+mfJV0g6SRJK83spFaf\nD0B3tfO+a7mkLe7+grvvk3SHpIs70xaAqdZO+BdJennC99uK+97DzFab2ZCZDY1ouI3dAeikKf/E\nxd3XuPuguw8OaMZU7w5Ak9oJ/3ZJiyd8f3RxH4ADQDvhf0TSUjM7zsymS7pU0j2daQvAVGt5qM/d\nR83sjyTdp/GhvnXu/nTHOkNP6O8rH49uxkwr/xEb8XiYccAqhil72FT+3RoeDOPvx8RcbY3zu/sG\nSRvaeQ4A9ch7ihWQHOEHkiL8QFKEH0iK8ANJEX4gqa5ez48aeHvj9Lr2Q2H5xI9fEdZ//bxnSmv/\nvuQ7rXR0QKgax79i+8dKa/d+/9fCbY96sLw2tjuef2EiXvmBpAg/kBThB5Ii/EBShB9IivADSZn7\nflwD2KZDbb4ze+9BxipmiQ5+vp6/eXm46U9WrAnrU3nZ7LCPhPUZNhDWj7vvsrB+wh88Wl5s45g+\n7Bv1uu9paupuXvmBpAg/kBThB5Ii/EBShB9IivADSRF+ICku6UWsYpXfvunxeHfj7fLVaE+8YXe4\n7fMX/TKsnzAQL/c+1sblzFXj+NtG94b1X/3HN8N61FnfjHhlKx8dLS8GpQ/sp/mHAjiYEH4gKcIP\nJEX4gaQIP5AU4QeSIvxAUm2N85vZVklvSBqTNOrug51oCj2kEV8z3xhufSzdd+wK6z8dnRvWTxho\nfd+NirWsq2YC2DJyaPyAzS/uX0MTROdGVOrWEt2Fs9391Q48D4Au4m0/kFS74XdJD5jZo2a2uhMN\nAeiOdt/2n+nu283sSEn3m9lz7v6exYSK/xRWS9JMzW5zdwA6pa1XfnffXnzdJelOSR+YkdHd17j7\noLsPDii+YAFA97QcfjObY2Zz37kt6TxJmzrVGICp1c7b/gWS7rTxaYanSfq6u9/bka4ATLmWw+/u\nL0iK1xIGAuF16R0QjeX3qamp7VvmY/H5EaE25u3fHwz1AUkRfiApwg8kRfiBpAg/kBThB5Ji6m6k\n1O4lvQcDXvmBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJ\nEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpyvCb2Toz22VmmybcN9/M7jezzcXXeVPb\nJoBOa+aV/1ZJ57/vvqskbXT3pZI2Ft8DOIBUht/dH5S05313XyxpfXF7vaQVHe4LwBRr9Xf+Be6+\no7j9iqQFHeoHQJe0/YGfu7tUvvCZma02syEzGxrRcLu7A9AhrYZ/p5ktlKTi666yB7r7GncfdPfB\nAc1ocXcAOq3V8N8jaVVxe5WkuzvTDoBuaWao73ZJ35e0zMy2mdllkq6TdK6ZbZb0ieJ7AAeQaVUP\ncPeVJaVzOtwLgC7iDD8gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBTh\nB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKVU3d3lVlcnjZQWvOxsfi5vRE/d39/\nxealK5JJjYp9Az2IV34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKpynN/M1kn6lKRd7n5Kcd81kj4r\naXfxsKvdfUNTe6wYy4/4yL6Wt6187tHR+AFR31V/Jw/OEQBq0swr/62Szp/k/hvd/dTiT3PBB9Az\nKsPv7g9K2tOFXgB0UTu/83/ezJ40s3VmNq9jHQHoilbD/xVJx0s6VdIOSdeXPdDMVpvZkJkNjWi4\nxd0B6LSWwu/uO919zN0bkm6RtDx47Bp3H3T3wQHNaLVPAB3WUvjNbOGEbz8taVNn2gHQLc0M9d0u\n6SxJR5jZNkl/KeksMztVkkvaKunyKewRwBSoDL+7r5zk7rUt7zEY8+6fF39u+OLnTiytLT73xXDb\nj/zKT8P6f249KawffW35myR/5KlwW84DQC/iDD8gKcIPJEX4gaQIP5AU4QeSIvxAUl2dutsGBjRt\nwVGl9WPviq8f2rDo5tLaiLc3ffaXP/y/Yf2urx9SWrvlE2eH246++HK8c4YCp0Sfyo9rQxxTXvmB\npAg/kBThB5Ii/EBShB9IivADSRF+IKmujvMPL56mzdceUVr/9qJ4EuC9jbdLawMWL7Fd5ReNeFrw\nFXPKa3/xmcXhtguvj8f5K5cHr5pWHJNiLD/GKz+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJNXVcf4l\nc17VuuX/FjwiHu+eZdPLt7T6/h+z9qYSSKtv9uywPtNGKp4h/nnhev4Yr/xAUoQfSIrwA0kRfiAp\nwg8kRfiBpAg/kFTlOL+ZLZZ0m6QFklzSGne/yczmS/qGpCWStkq6xN1/Hj3XbJNOm9EIHhGP27Yz\nll81r/8MGwjrf7W7fAnvRV99Lty26jQAHzuATxSo+jcJjvvoiceEmy4bGK7YeXyewFTa5+3NH9EL\nmknTqKQvuftJks6QdKWZnSTpKkkb3X2ppI3F9wAOEJXhd/cd7v5YcfsNSc9KWiTpYknri4etl7Ri\nqpoE0Hn79T7azJZIOk3Sw5IWuPuOovSKxn8tAHCAaDr8ZnaIpG9K+qK7vz6x5u4uTX6ytJmtNrMh\nMxt69WcH8O+2wEGmqfCb2YDGg/81d/9WcfdOM1tY1BdK2jXZtu6+xt0H3X3wiMMP/A9JgINFZfjN\nzCStlfSsu98woXSPpFXF7VWS7u58ewCmSjOX9H5M0u9KesrMHi/uu1rSdZL+w8wuk/SipEuqnuiN\nRp8eeGtuaf2i2eVTc0vSmEfDhLGqqb1fGt0b1r/3ud8ordnPnoh33lfxjqdx4P46ZAPxj5APl//d\ndp8WzIcuaV5/PJQ37PElv9Hw7WjFtlXDzrfuPDOs+3A46h0vy96lJdkrw+/uD0mlF0af09l2AHQL\nZ/gBSRF+ICnCDyRF+IGkCD+QFOEHkurq1N0v/XK+vvDDS0vrF511a7j9sJcvVV01jl91buFjwx+O\nt39ic2mt6uyDyiW4K7ZX1fkN0bhwNJ7cBJsWX+rs++KlzaP9n/w7z7TSUkeMVY2lVxy2HwwtC+tL\n9YP46YPj6iMVx7RDeOUHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaS6Os4/c0dDJ/zNW6X1jafH4+Hn\nzCpfovvNRntjoyvmxNfz//GXP1JaW3rlw+G2Plpx7fhUXr/d5nO32/vz/1I+D8J9S24Jt213uvXo\nev/ZfeU/S5L0+HA8bfiyta+F9apzP3phunZe+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKfMuzREu\nSYfafD/dymf73nvJGeH2G264sbR2WN+scNuq8wCq5gOI6pe9FM/h/sTa8nMEJOnI/550saN3+bYd\nYb3x5pulNRuIx7P7jl0U1nd/PJ7n4KOXPxXW1x7zUGmtahy/r+Ki+mh+B0maYeWnsVQt937a314R\n1o/8p/8J6zatYj2D0bj3Vj3sG/W672lqEgde+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqcpxfjNb\nLOk2SQs0PsX8Gne/ycyukfRZSbuLh17t7hui5zq0b76fMe2TpfWqsc/hC8qvDf/DG+8Mt/29Q18N\n62MVc+M3gtn1q84RqPLsvvJxekl65O1jwvqesUNKa3P73g63PWPWT8L6ydPj8yeq7G2U77/qevyq\ncf6qsfofj5TP0fCZv/vTcNsjb47H8dVX8W/eqOd6/f0Z529mMo9RSV9y98fMbK6kR83s/qJ2o7v/\nQ6uNAqhPZfjdfYekHcXtN8zsWUnxaWEAet5+/c5vZksknSbpnXmrPm9mT5rZOjObV7LNajMbMrOh\nEY+nRgLQPU2H38wOkfRNSV9099clfUXS8ZJO1fg7g+sn287d17j7oLsPDtiMDrQMoBOaCr+ZDWg8\n+F9z929JkrvvdPcxd29IukXS8qlrE0CnVYbfzEzSWknPuvsNE+5fOOFhn5a0qfPtAZgqzQz1nSnp\nu5Ke0v/PSHy1pJUaf8vvkrZKurz4cLBU1SW97Qyf9B8+P9x0y5/ESyr/9W/fEdYvnfvzsI7JRUOo\nVUN1PxyOpw1f+dDqsL7suvIh1LGnfxRuW9clue3q6FCfuz+kyVcrD8f0AfQ2zvADkiL8QFKEH0iK\n8ANJEX4gKcIPJNVTU3dXqjoPIFJxiWX/vEkvTXjXnovKzxPYeXY85vtbJz8X1j85P57++uTpr4T1\nBf3lY+mzKy6b3VMxpfmmfYeH9Q2vfTSs37vlxNLanO+WX4osSUd9e1tYH936UliXlQ93W3/8s9Sr\n4/hVmLobQCXCDyRF+IGkCD+QFOEHkiL8QFKEH0iqq+P8ZrZb0osT7jpCUjyndn16tbde7Uuit1Z1\nsrdj3f1DzTywq+H/wM7Nhtx9sLYGAr3aW6/2JdFbq+rqjbf9QFKEH0iq7vCvqXn/kV7trVf7kuit\nVbX0Vuvv/ADqU/crP4Ca1BJ+MzvfzH5kZlvM7Ko6eihjZlvN7Ckze9zMhmruZZ2Z7TKzTRPum29m\n95vZ5uJrfC1yd3u7xsy2F8fucTO7sKbeFpvZf5nZM2b2tJl9obi/1mMX9FXLcev6234z65f0vKRz\nJW2T9Iikle7+TFcbKWFmWyUNunvtY8Jm9puS9kq6zd1PKe77e0l73P264j/Oee7+Zz3S2zWS9ta9\ncnOxoMzCiStLS1oh6fdV47EL+rpENRy3Ol75l0va4u4vuPs+SXdIuriGPnqeuz8oac/77r5Y0vri\n9nqN//B0XUlvPcHdd7j7Y8XtNyS9s7J0rccu6KsWdYR/kaSXJ3y/Tb215LdLesDMHjWzeEmYeiyY\nsDLSK5IW1NnMJCpXbu6m960s3TPHrpUVrzuND/w+6Ex3P1XSBZKuLN7e9iQf/52tl4Zrmlq5uVsm\nWVn6XXUeu1ZXvO60OsK/XdLiCd8fXdzXE9x9e/F1l6Q71XurD+98Z5HU4uuumvt5Vy+t3DzZytLq\ngWPXSyte1xH+RyQtNbPjzGy6pEsl3VNDHx9gZnOKD2JkZnMknafeW334HkmriturJN1dYy/v0Ssr\nN5etLK2aj13PrXjt7l3/I+lCjX/i/2NJf15HDyV9HS/pieLP03X3Jul2jb8NHNH4ZyOXSTpc0kZJ\nmyU9IGl+D/X2VY2v5vykxoO2sKbeztT4W/onJT1e/Lmw7mMX9FXLceMMPyApPvADkiL8QFKEH0iK\n8ANJEX4gKcIPJEX4gaQIP5DU/wF3+TyBZYCTvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f36d2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKFAMP1Gg/h9X04YHAUT9dQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dd8fc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b6f8b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122aac2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npxh+Iqf+H259FKRxkuGSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ff6c0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11933fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxfATBYrhJwrU/wOyvtTPJAGdgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fe93c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogg/kCjCDySK8AOJ+j/XbG3GPIYMgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f496390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/CKJUvhFEqXwiyRK4RdJ1P8DcLmo/UF2R+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ffc9190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11003b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_show = 10\n",
    "\n",
    "for i in range(num_show):\n",
    "    dataset = train_datasets[np.random.choice(np.arange(num_classes))]\n",
    "    imgs = pickle.load( open( dataset, \"rb\" ) )\n",
    "    letter = str(os.path.splitext(dataset)[0][-1])\n",
    "    print ('\\n\\nLetter = %s \\n\\n' % letter)\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[np.random.choice(np.arange(len(imgs)))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ./notMNIST_large/A.pickle has 52909 images\n",
      "Dataset ./notMNIST_large/B.pickle has 52911 images\n",
      "Dataset ./notMNIST_large/C.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/D.pickle has 52911 images\n",
      "Dataset ./notMNIST_large/E.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/F.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/G.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/H.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/I.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/J.pickle has 52911 images\n",
      "Dataset ./notMNIST_large/A.pickle has 52909 images\n",
      "Dataset ./notMNIST_large/B.pickle has 52911 images\n",
      "Dataset ./notMNIST_large/C.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/D.pickle has 52911 images\n",
      "Dataset ./notMNIST_large/E.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/F.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/G.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/H.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/I.pickle has 52912 images\n",
      "Dataset ./notMNIST_large/J.pickle has 52911 images\n"
     ]
    }
   ],
   "source": [
    "min_imgs = np.inf\n",
    "for i in range(num_classes):\n",
    "    dataset = train_datasets[i]\n",
    "    imgs = pickle.load( open( dataset, \"rb\" ) )\n",
    "    if len(imgs) < min_imgs:\n",
    "        min_imgs = len(imgs) \n",
    "    print ('Dataset %s has %d images' % (dataset, len(imgs)))\n",
    "min_imgs\n",
    "for i in range(num_classes):\n",
    "    dataset = train_datasets[i]\n",
    "    imgs = pickle.load( open( dataset, \"rb\" ) )\n",
    "    if len(imgs) < min_imgs:\n",
    "        min_imgs = len(imgs) \n",
    "    print ('Dataset %s has %d images' % (dataset, len(imgs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800506\n"
     ]
    }
   ],
   "source": [
    "# Getting statistics of a file using os.stat(file_name)\n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.flags.writeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlaps between training and test sets: 1149. Execution time: 0.45802600000001803.\n",
      "Number of overlaps between training and validation sets: 976. Execution time: 0.4635319999999865.\n",
      "Number of overlaps between validation and test sets: 64. Execution time: 0.041805000000067594.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def check_overlaps(images1, images2):\n",
    "    images1.flags.writeable=False\n",
    "    images2.flags.writeable=False\n",
    "    start = time.clock()\n",
    "    hash1 = set([hash(image1.tobytes()) for image1 in images1])\n",
    "    hash2 = set([hash(image2.tobytes()) for image2 in images2])\n",
    "    all_overlaps = set.intersection(hash1, hash2)\n",
    "    return all_overlaps, time.clock()-start\n",
    "\n",
    "r, execTime = check_overlaps(train_dataset, test_dataset)    \n",
    "print('Number of overlaps between training and test sets: {}. Execution time: {}.'.format(len(r), execTime))\n",
    "\n",
    "r, execTime = check_overlaps(train_dataset, valid_dataset)   \n",
    "print('Number of overlaps between training and validation sets: {}. Execution time: {}.'.format(len(r), execTime))\n",
    "\n",
    "r, execTime = check_overlaps(valid_dataset, test_dataset) \n",
    "print('Number of overlaps between validation and test sets: {}. Execution time: {}.'.format(len(r), execTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,2,1],[1,3,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1],\n",
       "       [1, 3, 1]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 50\n",
      "Accuracy = 0.5975 \n",
      "Time taken = 0.34679293632507324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 100\n",
      "Accuracy = 0.7226 \n",
      "Time taken = 0.7031941413879395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1000\n",
      "Accuracy = 0.842 \n",
      "Time taken = 1.88157320022583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 5000\n",
      "Accuracy = 0.8392 \n",
      "Time taken = 12.945788145065308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   10.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn import metrics\n",
    "start = time.time()\n",
    "\n",
    "ns = [50, 100, 1000, 5000]\n",
    "for n in ns:\n",
    "\n",
    "    X_train = train_dataset[:n]\n",
    "    y_train = train_labels[:n]\n",
    "    train_n, width, height = X_train.shape\n",
    "    X_train = X_train.reshape((train_n, width * height))\n",
    "\n",
    "    X_test = test_dataset\n",
    "    y_test = test_labels\n",
    "    test_n, width, height = X_test.shape\n",
    "    X_test = X_test.reshape((test_n, width * height))\n",
    "\n",
    "    lr = LR(multi_class = 'multinomial', solver = 'lbfgs', verbose = 1, max_iter = 1000,random_state = 42, n_jobs = -1)\n",
    "    lr.fit(X_train, y_train)\n",
    "    predictions = lr.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, predictions)\n",
    "    print (\"N = {}\\nAccuracy = {} \\nTime taken = {}\".format(n,acc, time.time() - start))\n",
    "\n",
    "# n_list = [50,100,1000,5000]\n",
    "# # n = 50\n",
    "# for n in n_list:\n",
    "#     lr = LR(multi_class = 'multinomial', solver = 'lbfgs', n_jobs = -1)\n",
    "#     X_train = train_dataset[:n]\n",
    "#     y_train = train_labels[:n]\n",
    "#     n, k1, k2 = X_train.shape\n",
    "#     k = k1 * k2\n",
    "#     X_train = X_train.reshape(n, k)\n",
    "\n",
    "#     lr.fit(X, y)\n",
    "#     time.time() - start\n",
    "\n",
    "#     X_test = test_dataset\n",
    "#     y_test = test_labels\n",
    "#     n1, k1, k2 = X_test.shape\n",
    "#     X_test = X_test.reshape(n1, k)\n",
    "\n",
    "#     logits = lr.predict(X_test)\n",
    "#     acc = metrics.accuracy_score(y_test, logits)\n",
    "#     print ('Acc = {}'.format( acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset = 10000 #Batch size, I think\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 16.979116\n",
      "Training accuracy: 8.5%\n",
      "Validation accuracy: 11.7%\n",
      "Loss at step 100: 2.339536\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 70.6%\n",
      "Loss at step 200: 1.874211\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 73.0%\n",
      "Loss at step 300: 1.625849\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 400: 1.460141\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 74.3%\n",
      "Loss at step 500: 1.337100\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 600: 1.240168\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 700: 1.160873\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 800: 1.094540\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 75.5%\n",
      "Test accuracy: 82.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.369751\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 17.2%\n",
      "Minibatch loss at step 500: 2.359363\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 1000: 1.492037\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 1500: 1.291395\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 2000: 1.036627\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 2500: 1.260880\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 3000: 0.891324\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 86.4%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 479.485657\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 35.8%\n",
      "Minibatch loss at step 500: 81.473755\n",
      "Minibatch accuracy: 58.6%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 1000: 41.340561\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 1500: 33.019066\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 2000: 28.761848\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 2500: 40.956177\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 3000: 18.476761\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 69.9%\n",
      "Test accuracy: 76.0%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    logits = tf.matmul(h1, weights1) + biases1\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases, weights1) + biases1))\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases, weights1) + biases1))\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 358.088013\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 32.1%\n",
      "Minibatch loss at step 500: 90.119164\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 1000: 6.512262\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 5.539775\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 19.876312\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2500: 6.264662\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 1.903631\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Model saved in file: notMNIST.ckpt\n",
      "Test accuracy: 87.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    logits = tf.matmul(h1, weights1) + biases1\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), weights1) + biases1)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  saver = tf.train.Saver()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  save_path = saver.save(session, 'notMNIST.ckpt')\n",
    "  print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model recovered\n",
      "Accuracy for new preds = 86.46\n",
      "Test accuracy: 87.4%\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "batch_n = 5000\n",
    "new_dataset = test_dataset[:batch_n] + np.random.normal(scale = .05, size = test_dataset[:batch_n].shape)\n",
    "new_labels = test_labels[:batch_n]\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_new_dataset = tf.constant(new_dataset, dtype = tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), weights1) + biases1)\n",
    "    new_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_new_dataset, weights) + biases), weights1) + biases1)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './notMNIST.ckpt')\n",
    "    print ('model recovered')\n",
    "    \n",
    "    preds = new_prediction.eval()\n",
    "    print ('Accuracy for new preds = {}'.format(accuracy(preds, new_labels)))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction[:batch_n].eval(), test_labels[:batch_n]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./notMNIST_large.tar.gz\n",
      "Found and verified ./notMNIST_small.tar.gz\n",
      "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.tar.gz.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.tar.gz.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n",
      "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/J.pickle already present - Skipping pickling.\n",
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# # These are all the modules we'll be using later. Make sure you can import them\n",
    "# # before proceeding further.\n",
    "# from __future__ import print_function\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import sys\n",
    "# import tarfile\n",
    "# from IPython.display import display, Image\n",
    "# from scipy import ndimage\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from six.moves.urllib.request import urlretrieve\n",
    "# from six.moves import cPickle as pickle\n",
    "# import itertools\n",
    "\n",
    "# # Config the matplotlib backend as plotting inline in IPython\n",
    "# %matplotlib inline\n",
    "\n",
    "# url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "# last_percent_reported = None\n",
    "# data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "# def download_progress_hook(count, blockSize, totalSize):\n",
    "#   \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "#   slow internet connections. Reports every 5% change in download progress.\n",
    "#   \"\"\"\n",
    "#   global last_percent_reported\n",
    "#   percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "#   if last_percent_reported != percent:\n",
    "#     if percent % 5 == 0:\n",
    "#       sys.stdout.write(\"%s%%\" % percent)\n",
    "#       sys.stdout.flush()\n",
    "#     else:\n",
    "#       sys.stdout.write(\".\")\n",
    "#       sys.stdout.flush()\n",
    "      \n",
    "#     last_percent_reported = percent\n",
    "        \n",
    "# def maybe_download(filename, expected_bytes, force=False):\n",
    "#   \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "#   dest_filename = os.path.join(data_root, filename)\n",
    "#   if force or not os.path.exists(dest_filename):\n",
    "#     print('Attempting to download:', filename) \n",
    "#     filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "#     print('\\nDownload Complete!')\n",
    "#   statinfo = os.stat(dest_filename)\n",
    "#   if statinfo.st_size == expected_bytes:\n",
    "#     print('Found and verified', dest_filename)\n",
    "#   else:\n",
    "#     raise Exception(\n",
    "#       'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "#   return dest_filename\n",
    "\n",
    "# train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "# test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "\n",
    "# num_classes = 10\n",
    "# np.random.seed(133)\n",
    "\n",
    "# def maybe_extract(filename, force=False):\n",
    "#   root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "#   if os.path.isdir(root) and not force: #See if root is an existing directory and if force = False\n",
    "#     # You may override by setting force=True.\n",
    "#     print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "#   else:\n",
    "#     print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "#     tar = tarfile.open(filename)\n",
    "#     sys.stdout.flush()\n",
    "#     tar.extractall(data_root)\n",
    "#     tar.close()\n",
    "#   data_folders = [\n",
    "#     os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "#     if os.path.isdir(os.path.join(root, d))]\n",
    "#   if len(data_folders) != num_classes:\n",
    "#     raise Exception(\n",
    "#       'Expected %d folders, one per class. Found %d instead.' % (\n",
    "#         num_classes, len(data_folders)))\n",
    "#   print(data_folders)\n",
    "#   return data_folders\n",
    "  \n",
    "# train_folders = maybe_extract(train_filename)\n",
    "# test_folders = maybe_extract(test_filename)\n",
    "\n",
    "# image_size = 28  # Pixel width and height.\n",
    "# pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "# def load_letter(folder, min_num_images):\n",
    "#   \"\"\"Load the data for a single letter label.\"\"\"\n",
    "#   image_files = os.listdir(folder)\n",
    "#   dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "#                          dtype=np.float32)\n",
    "#   print(folder)\n",
    "#   num_images = 0\n",
    "#   for image in image_files:\n",
    "#     image_file = os.path.join(folder, image)\n",
    "#     try:\n",
    "#       image_data = (ndimage.imread(image_file).astype(float) - \n",
    "#                     pixel_depth / 2) / pixel_depth\n",
    "#       if image_data.shape != (image_size, image_size):\n",
    "#         raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "#       dataset[num_images, :, :] = image_data\n",
    "#       num_images = num_images + 1\n",
    "#     except IOError as e:\n",
    "#       print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "#   dataset = dataset[0:num_images, :, :]\n",
    "#   if num_images < min_num_images:\n",
    "#     raise Exception('Many fewer images than expected: %d < %d' %\n",
    "#                     (num_images, min_num_images))\n",
    "    \n",
    "#   print('Full dataset tensor:', dataset.shape)\n",
    "#   print('Mean:', np.mean(dataset))\n",
    "#   print('Standard deviation:', np.std(dataset))\n",
    "#   return dataset\n",
    "        \n",
    "# def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "#   dataset_names = []\n",
    "#   for folder in data_folders:\n",
    "#     set_filename = folder + '.pickle'\n",
    "#     dataset_names.append(set_filename)\n",
    "#     if os.path.exists(set_filename) and not force:\n",
    "#       # You may override by setting force=True.\n",
    "#       print('%s already present - Skipping pickling.' % set_filename)\n",
    "#     else:\n",
    "#       print('Pickling %s.' % set_filename)\n",
    "#       dataset = load_letter(folder, min_num_images_per_class)\n",
    "#       try:\n",
    "#         with open(set_filename, 'wb') as f:\n",
    "#           pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "#       except Exception as e:\n",
    "#         print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "#   return dataset_names\n",
    "\n",
    "# train_datasets = maybe_pickle(train_folders, 45000)\n",
    "# test_datasets = maybe_pickle(test_folders, 1800)\n",
    "\n",
    "# def make_arrays(nb_rows, img_size):\n",
    "#   if nb_rows:\n",
    "#     dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "#     labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "#   else:\n",
    "#     dataset, labels = None, None\n",
    "#   return dataset, labels\n",
    "\n",
    "# def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "#   num_classes = len(pickle_files)\n",
    "#   valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "#   train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "#   vsize_per_class = valid_size // num_classes\n",
    "#   tsize_per_class = train_size // num_classes\n",
    "    \n",
    "#   start_v, start_t = 0, 0\n",
    "#   end_v, end_t = vsize_per_class, tsize_per_class\n",
    "#   end_l = vsize_per_class+tsize_per_class\n",
    "#   for label, pickle_file in enumerate(pickle_files):       \n",
    "#     try:\n",
    "#       with open(pickle_file, 'rb') as f:\n",
    "#         letter_set = pickle.load(f)\n",
    "#         # let's shuffle the letters to have random validation and training set\n",
    "#         np.random.shuffle(letter_set)\n",
    "#         if valid_dataset is not None:\n",
    "#           valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "#           valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "#           valid_labels[start_v:end_v] = label\n",
    "#           start_v += vsize_per_class\n",
    "#           end_v += vsize_per_class\n",
    "                    \n",
    "#         train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "#         train_dataset[start_t:end_t, :, :] = train_letter\n",
    "#         train_labels[start_t:end_t] = label\n",
    "#         start_t += tsize_per_class\n",
    "#         end_t += tsize_per_class\n",
    "#     except Exception as e:\n",
    "#       print('Unable to process data from', pickle_file, ':', e)\n",
    "#       raise\n",
    "    \n",
    "#   return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "# train_size = 200000\n",
    "# valid_size = 10000\n",
    "# test_size = 10000\n",
    "\n",
    "# valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "#   train_datasets, train_size, valid_size)\n",
    "# _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "# print('Training:', train_dataset.shape, train_labels.shape)\n",
    "# print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "# print('Testing:', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sequence_length = 10\n",
    "state_dim = 128\n",
    "num_layers = 2\n",
    "tf.reset_default_graph()\n",
    "\n",
    "in_ph\n",
    "targ_ph\n",
    "\n",
    "cell1 = rnn.BasicLSTMCell(state_dim)\n",
    "cell2 = rnn.BasicLSTMCell(state_dim)\n",
    "multicell = rnn.MultiRNNCell( [cell1,cell2], state_is_tuple=True)\n",
    "initial_state = multicell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "rnn_out, final_state = legacy_seq2seq.rnn_decoder(inputs, initial_state, multicell)\n",
    "logits = []\n",
    "W = tf.Variable(tf.truncated_normal([state_dim, vocab_size]), dtype = tf.float32)\n",
    "b = tf.Variable(tf.ones(vocab_size)*0.1, dtype = tf.float32)\n",
    "for rnn in rnn_out:\n",
    "    logit = tf.matmul(rnn, W) + b\n",
    "    logits.append(logit)\n",
    "\n",
    "ones_list = []\n",
    "for i in xrange(batch_size):\n",
    "    ones_list.append(1.)\n",
    "loss = legacy_seq2seq.sequence_loss(logits, targets, ones_list)\n",
    "\n",
    "optim = tf.train.AdamOptimizer(.001).minimize(loss)\n",
    "\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "\n",
    "\n",
    "s_initial_state = multicell.zero_state(1, tf.float32)\n",
    "s_in_ph = tf.placeholder( tf.int32, [ 1 ], name='s_in_ph' )\n",
    "s_inputs = tf.one_hot( s_in_ph, vocab_size, name=\"s_inputs\" )\n",
    "s_rnn_out, s_final_state = legacy_seq2seq.rnn_decoder([s_inputs,], s_initial_state, multicell)\n",
    "\n",
    "s_logits = tf.matmul(s_rnn_out[0], W) + b\n",
    "s_probs = tf.nn.softmax(s_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1 = [1,2,3]\n",
    "list2 = [4,5,6]\n",
    "list1.extend(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Global config variables\n",
    "num_steps = 10 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 5\n",
    "state_size = 16\n",
    "learning_rate = 0.1\n",
    "n_time_series = 4\n",
    "\n",
    "def gen_data(n_periods=1000000, n_time_series = n_time_series):\n",
    "    \"\"\"\n",
    "    Generates n_time_series time series that each last for n_periods periods along with a single \n",
    "    time series that lasts n_periods periods. This data generator was built for a neural net that maps \n",
    "    panel data to a single dependent variable. \n",
    "    \n",
    "    args:\n",
    "        \n",
    "        n_periods: scalar, number of periods desired/total data points generated\n",
    "        n_time_series: scalar, number of time series desired for panel data\n",
    "        \n",
    "    returns:\n",
    "    \n",
    "        X: randomly generated panel data\n",
    "        Y: the sum of all panel data\n",
    "    \"\"\"\n",
    "    X = np.random.randint(0,10,(n_time_series,n_periods))\n",
    "    Y = np.sum(X, axis = 0)   \n",
    "    return X, Y\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    #Raw x and y, X n_time_series by n_periods, Y n_periods long\n",
    "    raw_x, raw_y = raw_data\n",
    "    print (raw_x)\n",
    "    print (raw_y)\n",
    "    data_length = len(raw_y)\n",
    "#     print (data_length)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    # batch partition length is how long each series must be in order for all series to be the same length given \n",
    "    # a data_length and batch size. \n",
    "    batch_partition_length = data_length // batch_size\n",
    "    # Empty matrices to fill with data\n",
    "    data_x = np.zeros([batch_size, batch_partition_length, n_time_series], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        #Fill each row of data_x and data_y with the appropriate number of \n",
    "        data_x[i] = raw_x[:, batch_partition_length * i:batch_partition_length * (i + 1)].T\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    # Each of these are just truncated sequences, split up along the first axis, which is the time dimension.\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    \n",
    "    #epoch size is 500\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 9 5 ..., 5 2 9]\n",
      " [6 3 0 ..., 0 0 7]\n",
      " [0 1 7 ..., 7 2 6]\n",
      " [9 7 0 ..., 3 1 0]]\n",
      "[16 20 12 ..., 15  5 22]\n",
      "(200, 10, 4) (200, 10)\n",
      "0\n",
      "(200, 10, 4) (200, 10)\n",
      "1\n",
      "(200, 10, 4) (200, 10)\n",
      "2\n",
      "(200, 10, 4) (200, 10)\n",
      "3\n",
      "(200, 10, 4) (200, 10)\n",
      "4\n",
      "(200, 10, 4) (200, 10)\n",
      "5\n",
      "(200, 10, 4) (200, 10)\n",
      "6\n",
      "(200, 10, 4) (200, 10)\n",
      "7\n",
      "(200, 10, 4) (200, 10)\n",
      "8\n",
      "(200, 10, 4) (200, 10)\n",
      "9\n",
      "(200, 10, 4) (200, 10)\n",
      "10\n",
      "(200, 10, 4) (200, 10)\n",
      "11\n",
      "(200, 10, 4) (200, 10)\n",
      "12\n",
      "(200, 10, 4) (200, 10)\n",
      "13\n",
      "(200, 10, 4) (200, 10)\n",
      "14\n",
      "(200, 10, 4) (200, 10)\n",
      "15\n",
      "(200, 10, 4) (200, 10)\n",
      "16\n",
      "(200, 10, 4) (200, 10)\n",
      "17\n",
      "(200, 10, 4) (200, 10)\n",
      "18\n",
      "(200, 10, 4) (200, 10)\n",
      "19\n",
      "(200, 10, 4) (200, 10)\n",
      "20\n",
      "(200, 10, 4) (200, 10)\n",
      "21\n",
      "(200, 10, 4) (200, 10)\n",
      "22\n",
      "(200, 10, 4) (200, 10)\n",
      "23\n",
      "(200, 10, 4) (200, 10)\n",
      "24\n",
      "(200, 10, 4) (200, 10)\n",
      "25\n",
      "(200, 10, 4) (200, 10)\n",
      "26\n",
      "(200, 10, 4) (200, 10)\n",
      "27\n",
      "(200, 10, 4) (200, 10)\n",
      "28\n",
      "(200, 10, 4) (200, 10)\n",
      "29\n",
      "(200, 10, 4) (200, 10)\n",
      "30\n",
      "(200, 10, 4) (200, 10)\n",
      "31\n",
      "(200, 10, 4) (200, 10)\n",
      "32\n",
      "(200, 10, 4) (200, 10)\n",
      "33\n",
      "(200, 10, 4) (200, 10)\n",
      "34\n",
      "(200, 10, 4) (200, 10)\n",
      "35\n",
      "(200, 10, 4) (200, 10)\n",
      "36\n",
      "(200, 10, 4) (200, 10)\n",
      "37\n",
      "(200, 10, 4) (200, 10)\n",
      "38\n",
      "(200, 10, 4) (200, 10)\n",
      "39\n",
      "(200, 10, 4) (200, 10)\n",
      "40\n",
      "(200, 10, 4) (200, 10)\n",
      "41\n",
      "(200, 10, 4) (200, 10)\n",
      "42\n",
      "(200, 10, 4) (200, 10)\n",
      "43\n",
      "(200, 10, 4) (200, 10)\n",
      "44\n",
      "(200, 10, 4) (200, 10)\n",
      "45\n",
      "(200, 10, 4) (200, 10)\n",
      "46\n",
      "(200, 10, 4) (200, 10)\n",
      "47\n",
      "(200, 10, 4) (200, 10)\n",
      "48\n",
      "(200, 10, 4) (200, 10)\n",
      "49\n",
      "(200, 10, 4) (200, 10)\n",
      "50\n",
      "(200, 10, 4) (200, 10)\n",
      "51\n",
      "(200, 10, 4) (200, 10)\n",
      "52\n",
      "(200, 10, 4) (200, 10)\n",
      "53\n",
      "(200, 10, 4) (200, 10)\n",
      "54\n",
      "(200, 10, 4) (200, 10)\n",
      "55\n",
      "(200, 10, 4) (200, 10)\n",
      "56\n",
      "(200, 10, 4) (200, 10)\n",
      "57\n",
      "(200, 10, 4) (200, 10)\n",
      "58\n",
      "(200, 10, 4) (200, 10)\n",
      "59\n",
      "(200, 10, 4) (200, 10)\n",
      "60\n",
      "(200, 10, 4) (200, 10)\n",
      "61\n",
      "(200, 10, 4) (200, 10)\n",
      "62\n",
      "(200, 10, 4) (200, 10)\n",
      "63\n",
      "(200, 10, 4) (200, 10)\n",
      "64\n",
      "(200, 10, 4) (200, 10)\n",
      "65\n",
      "(200, 10, 4) (200, 10)\n",
      "66\n",
      "(200, 10, 4) (200, 10)\n",
      "67\n",
      "(200, 10, 4) (200, 10)\n",
      "68\n",
      "(200, 10, 4) (200, 10)\n",
      "69\n",
      "(200, 10, 4) (200, 10)\n",
      "70\n",
      "(200, 10, 4) (200, 10)\n",
      "71\n",
      "(200, 10, 4) (200, 10)\n",
      "72\n",
      "(200, 10, 4) (200, 10)\n",
      "73\n",
      "(200, 10, 4) (200, 10)\n",
      "74\n",
      "(200, 10, 4) (200, 10)\n",
      "75\n",
      "(200, 10, 4) (200, 10)\n",
      "76\n",
      "(200, 10, 4) (200, 10)\n",
      "77\n",
      "(200, 10, 4) (200, 10)\n",
      "78\n",
      "(200, 10, 4) (200, 10)\n",
      "79\n",
      "(200, 10, 4) (200, 10)\n",
      "80\n",
      "(200, 10, 4) (200, 10)\n",
      "81\n",
      "(200, 10, 4) (200, 10)\n",
      "82\n",
      "(200, 10, 4) (200, 10)\n",
      "83\n",
      "(200, 10, 4) (200, 10)\n",
      "84\n",
      "(200, 10, 4) (200, 10)\n",
      "85\n",
      "(200, 10, 4) (200, 10)\n",
      "86\n",
      "(200, 10, 4) (200, 10)\n",
      "87\n",
      "(200, 10, 4) (200, 10)\n",
      "88\n",
      "(200, 10, 4) (200, 10)\n",
      "89\n",
      "(200, 10, 4) (200, 10)\n",
      "90\n",
      "(200, 10, 4) (200, 10)\n",
      "91\n",
      "(200, 10, 4) (200, 10)\n",
      "92\n",
      "(200, 10, 4) (200, 10)\n",
      "93\n",
      "(200, 10, 4) (200, 10)\n",
      "94\n",
      "(200, 10, 4) (200, 10)\n",
      "95\n",
      "(200, 10, 4) (200, 10)\n",
      "96\n",
      "(200, 10, 4) (200, 10)\n",
      "97\n",
      "(200, 10, 4) (200, 10)\n",
      "98\n",
      "(200, 10, 4) (200, 10)\n",
      "99\n",
      "(200, 10, 4) (200, 10)\n",
      "100\n",
      "(200, 10, 4) (200, 10)\n",
      "101\n",
      "(200, 10, 4) (200, 10)\n",
      "102\n",
      "(200, 10, 4) (200, 10)\n",
      "103\n",
      "(200, 10, 4) (200, 10)\n",
      "104\n",
      "(200, 10, 4) (200, 10)\n",
      "105\n",
      "(200, 10, 4) (200, 10)\n",
      "106\n",
      "(200, 10, 4) (200, 10)\n",
      "107\n",
      "(200, 10, 4) (200, 10)\n",
      "108\n",
      "(200, 10, 4) (200, 10)\n",
      "109\n",
      "(200, 10, 4) (200, 10)\n",
      "110\n",
      "(200, 10, 4) (200, 10)\n",
      "111\n",
      "(200, 10, 4) (200, 10)\n",
      "112\n",
      "(200, 10, 4) (200, 10)\n",
      "113\n",
      "(200, 10, 4) (200, 10)\n",
      "114\n",
      "(200, 10, 4) (200, 10)\n",
      "115\n",
      "(200, 10, 4) (200, 10)\n",
      "116\n",
      "(200, 10, 4) (200, 10)\n",
      "117\n",
      "(200, 10, 4) (200, 10)\n",
      "118\n",
      "(200, 10, 4) (200, 10)\n",
      "119\n",
      "(200, 10, 4) (200, 10)\n",
      "120\n",
      "(200, 10, 4) (200, 10)\n",
      "121\n",
      "(200, 10, 4) (200, 10)\n",
      "122\n",
      "(200, 10, 4) (200, 10)\n",
      "123\n",
      "(200, 10, 4) (200, 10)\n",
      "124\n",
      "(200, 10, 4) (200, 10)\n",
      "125\n",
      "(200, 10, 4) (200, 10)\n",
      "126\n",
      "(200, 10, 4) (200, 10)\n",
      "127\n",
      "(200, 10, 4) (200, 10)\n",
      "128\n",
      "(200, 10, 4) (200, 10)\n",
      "129\n",
      "(200, 10, 4) (200, 10)\n",
      "130\n",
      "(200, 10, 4) (200, 10)\n",
      "131\n",
      "(200, 10, 4) (200, 10)\n",
      "132\n",
      "(200, 10, 4) (200, 10)\n",
      "133\n",
      "(200, 10, 4) (200, 10)\n",
      "134\n",
      "(200, 10, 4) (200, 10)\n",
      "135\n",
      "(200, 10, 4) (200, 10)\n",
      "136\n",
      "(200, 10, 4) (200, 10)\n",
      "137\n",
      "(200, 10, 4) (200, 10)\n",
      "138\n",
      "(200, 10, 4) (200, 10)\n",
      "139\n",
      "(200, 10, 4) (200, 10)\n",
      "140\n",
      "(200, 10, 4) (200, 10)\n",
      "141\n",
      "(200, 10, 4) (200, 10)\n",
      "142\n",
      "(200, 10, 4) (200, 10)\n",
      "143\n",
      "(200, 10, 4) (200, 10)\n",
      "144\n",
      "(200, 10, 4) (200, 10)\n",
      "145\n",
      "(200, 10, 4) (200, 10)\n",
      "146\n",
      "(200, 10, 4) (200, 10)\n",
      "147\n",
      "(200, 10, 4) (200, 10)\n",
      "148\n",
      "(200, 10, 4) (200, 10)\n",
      "149\n",
      "(200, 10, 4) (200, 10)\n",
      "150\n",
      "(200, 10, 4) (200, 10)\n",
      "151\n",
      "(200, 10, 4) (200, 10)\n",
      "152\n",
      "(200, 10, 4) (200, 10)\n",
      "153\n",
      "(200, 10, 4) (200, 10)\n",
      "154\n",
      "(200, 10, 4) (200, 10)\n",
      "155\n",
      "(200, 10, 4) (200, 10)\n",
      "156\n",
      "(200, 10, 4) (200, 10)\n",
      "157\n",
      "(200, 10, 4) (200, 10)\n",
      "158\n",
      "(200, 10, 4) (200, 10)\n",
      "159\n",
      "(200, 10, 4) (200, 10)\n",
      "160\n",
      "(200, 10, 4) (200, 10)\n",
      "161\n",
      "(200, 10, 4) (200, 10)\n",
      "162\n",
      "(200, 10, 4) (200, 10)\n",
      "163\n",
      "(200, 10, 4) (200, 10)\n",
      "164\n",
      "(200, 10, 4) (200, 10)\n",
      "165\n",
      "(200, 10, 4) (200, 10)\n",
      "166\n",
      "(200, 10, 4) (200, 10)\n",
      "167\n",
      "(200, 10, 4) (200, 10)\n",
      "168\n",
      "(200, 10, 4) (200, 10)\n",
      "169\n",
      "(200, 10, 4) (200, 10)\n",
      "170\n",
      "(200, 10, 4) (200, 10)\n",
      "171\n",
      "(200, 10, 4) (200, 10)\n",
      "172\n",
      "(200, 10, 4) (200, 10)\n",
      "173\n",
      "(200, 10, 4) (200, 10)\n",
      "174\n",
      "(200, 10, 4) (200, 10)\n",
      "175\n",
      "(200, 10, 4) (200, 10)\n",
      "176\n",
      "(200, 10, 4) (200, 10)\n",
      "177\n",
      "(200, 10, 4) (200, 10)\n",
      "178\n",
      "(200, 10, 4) (200, 10)\n",
      "179\n",
      "(200, 10, 4) (200, 10)\n",
      "180\n",
      "(200, 10, 4) (200, 10)\n",
      "181\n",
      "(200, 10, 4) (200, 10)\n",
      "182\n",
      "(200, 10, 4) (200, 10)\n",
      "183\n",
      "(200, 10, 4) (200, 10)\n",
      "184\n",
      "(200, 10, 4) (200, 10)\n",
      "185\n",
      "(200, 10, 4) (200, 10)\n",
      "186\n",
      "(200, 10, 4) (200, 10)\n",
      "187\n",
      "(200, 10, 4) (200, 10)\n",
      "188\n",
      "(200, 10, 4) (200, 10)\n",
      "189\n",
      "(200, 10, 4) (200, 10)\n",
      "190\n",
      "(200, 10, 4) (200, 10)\n",
      "191\n",
      "(200, 10, 4) (200, 10)\n",
      "192\n",
      "(200, 10, 4) (200, 10)\n",
      "193\n",
      "(200, 10, 4) (200, 10)\n",
      "194\n",
      "(200, 10, 4) (200, 10)\n",
      "195\n",
      "(200, 10, 4) (200, 10)\n",
      "196\n",
      "(200, 10, 4) (200, 10)\n",
      "197\n",
      "(200, 10, 4) (200, 10)\n",
      "198\n",
      "(200, 10, 4) (200, 10)\n",
      "199\n",
      "(200, 10, 4) (200, 10)\n",
      "200\n",
      "(200, 10, 4) (200, 10)\n",
      "201\n",
      "(200, 10, 4) (200, 10)\n",
      "202\n",
      "(200, 10, 4) (200, 10)\n",
      "203\n",
      "(200, 10, 4) (200, 10)\n",
      "204\n",
      "(200, 10, 4) (200, 10)\n",
      "205\n",
      "(200, 10, 4) (200, 10)\n",
      "206\n",
      "(200, 10, 4) (200, 10)\n",
      "207\n",
      "(200, 10, 4) (200, 10)\n",
      "208\n",
      "(200, 10, 4) (200, 10)\n",
      "209\n",
      "(200, 10, 4) (200, 10)\n",
      "210\n",
      "(200, 10, 4) (200, 10)\n",
      "211\n",
      "(200, 10, 4) (200, 10)\n",
      "212\n",
      "(200, 10, 4) (200, 10)\n",
      "213\n",
      "(200, 10, 4) (200, 10)\n",
      "214\n",
      "(200, 10, 4) (200, 10)\n",
      "215\n",
      "(200, 10, 4) (200, 10)\n",
      "216\n",
      "(200, 10, 4) (200, 10)\n",
      "217\n",
      "(200, 10, 4) (200, 10)\n",
      "218\n",
      "(200, 10, 4) (200, 10)\n",
      "219\n",
      "(200, 10, 4) (200, 10)\n",
      "220\n",
      "(200, 10, 4) (200, 10)\n",
      "221\n",
      "(200, 10, 4) (200, 10)\n",
      "222\n",
      "(200, 10, 4) (200, 10)\n",
      "223\n",
      "(200, 10, 4) (200, 10)\n",
      "224\n",
      "(200, 10, 4) (200, 10)\n",
      "225\n",
      "(200, 10, 4) (200, 10)\n",
      "226\n",
      "(200, 10, 4) (200, 10)\n",
      "227\n",
      "(200, 10, 4) (200, 10)\n",
      "228\n",
      "(200, 10, 4) (200, 10)\n",
      "229\n",
      "(200, 10, 4) (200, 10)\n",
      "230\n",
      "(200, 10, 4) (200, 10)\n",
      "231\n",
      "(200, 10, 4) (200, 10)\n",
      "232\n",
      "(200, 10, 4) (200, 10)\n",
      "233\n",
      "(200, 10, 4) (200, 10)\n",
      "234\n",
      "(200, 10, 4) (200, 10)\n",
      "235\n",
      "(200, 10, 4) (200, 10)\n",
      "236\n",
      "(200, 10, 4) (200, 10)\n",
      "237\n",
      "(200, 10, 4) (200, 10)\n",
      "238\n",
      "(200, 10, 4) (200, 10)\n",
      "239\n",
      "(200, 10, 4) (200, 10)\n",
      "240\n",
      "(200, 10, 4) (200, 10)\n",
      "241\n",
      "(200, 10, 4) (200, 10)\n",
      "242\n",
      "(200, 10, 4) (200, 10)\n",
      "243\n",
      "(200, 10, 4) (200, 10)\n",
      "244\n",
      "(200, 10, 4) (200, 10)\n",
      "245\n",
      "(200, 10, 4) (200, 10)\n",
      "246\n",
      "(200, 10, 4) (200, 10)\n",
      "247\n",
      "(200, 10, 4) (200, 10)\n",
      "248\n",
      "(200, 10, 4) (200, 10)\n",
      "249\n",
      "(200, 10, 4) (200, 10)\n",
      "250\n",
      "(200, 10, 4) (200, 10)\n",
      "251\n",
      "(200, 10, 4) (200, 10)\n",
      "252\n",
      "(200, 10, 4) (200, 10)\n",
      "253\n",
      "(200, 10, 4) (200, 10)\n",
      "254\n",
      "(200, 10, 4) (200, 10)\n",
      "255\n",
      "(200, 10, 4) (200, 10)\n",
      "256\n",
      "(200, 10, 4) (200, 10)\n",
      "257\n",
      "(200, 10, 4) (200, 10)\n",
      "258\n",
      "(200, 10, 4) (200, 10)\n",
      "259\n",
      "(200, 10, 4) (200, 10)\n",
      "260\n",
      "(200, 10, 4) (200, 10)\n",
      "261\n",
      "(200, 10, 4) (200, 10)\n",
      "262\n",
      "(200, 10, 4) (200, 10)\n",
      "263\n",
      "(200, 10, 4) (200, 10)\n",
      "264\n",
      "(200, 10, 4) (200, 10)\n",
      "265\n",
      "(200, 10, 4) (200, 10)\n",
      "266\n",
      "(200, 10, 4) (200, 10)\n",
      "267\n",
      "(200, 10, 4) (200, 10)\n",
      "268\n",
      "(200, 10, 4) (200, 10)\n",
      "269\n",
      "(200, 10, 4) (200, 10)\n",
      "270\n",
      "(200, 10, 4) (200, 10)\n",
      "271\n",
      "(200, 10, 4) (200, 10)\n",
      "272\n",
      "(200, 10, 4) (200, 10)\n",
      "273\n",
      "(200, 10, 4) (200, 10)\n",
      "274\n",
      "(200, 10, 4) (200, 10)\n",
      "275\n",
      "(200, 10, 4) (200, 10)\n",
      "276\n",
      "(200, 10, 4) (200, 10)\n",
      "277\n",
      "(200, 10, 4) (200, 10)\n",
      "278\n",
      "(200, 10, 4) (200, 10)\n",
      "279\n",
      "(200, 10, 4) (200, 10)\n",
      "280\n",
      "(200, 10, 4) (200, 10)\n",
      "281\n",
      "(200, 10, 4) (200, 10)\n",
      "282\n",
      "(200, 10, 4) (200, 10)\n",
      "283\n",
      "(200, 10, 4) (200, 10)\n",
      "284\n",
      "(200, 10, 4) (200, 10)\n",
      "285\n",
      "(200, 10, 4) (200, 10)\n",
      "286\n",
      "(200, 10, 4) (200, 10)\n",
      "287\n",
      "(200, 10, 4) (200, 10)\n",
      "288\n",
      "(200, 10, 4) (200, 10)\n",
      "289\n",
      "(200, 10, 4) (200, 10)\n",
      "290\n",
      "(200, 10, 4) (200, 10)\n",
      "291\n",
      "(200, 10, 4) (200, 10)\n",
      "292\n",
      "(200, 10, 4) (200, 10)\n",
      "293\n",
      "(200, 10, 4) (200, 10)\n",
      "294\n",
      "(200, 10, 4) (200, 10)\n",
      "295\n",
      "(200, 10, 4) (200, 10)\n",
      "296\n",
      "(200, 10, 4) (200, 10)\n",
      "297\n",
      "(200, 10, 4) (200, 10)\n",
      "298\n",
      "(200, 10, 4) (200, 10)\n",
      "299\n",
      "(200, 10, 4) (200, 10)\n",
      "300\n",
      "(200, 10, 4) (200, 10)\n",
      "301\n",
      "(200, 10, 4) (200, 10)\n",
      "302\n",
      "(200, 10, 4) (200, 10)\n",
      "303\n",
      "(200, 10, 4) (200, 10)\n",
      "304\n",
      "(200, 10, 4) (200, 10)\n",
      "305\n",
      "(200, 10, 4) (200, 10)\n",
      "306\n",
      "(200, 10, 4) (200, 10)\n",
      "307\n",
      "(200, 10, 4) (200, 10)\n",
      "308\n",
      "(200, 10, 4) (200, 10)\n",
      "309\n",
      "(200, 10, 4) (200, 10)\n",
      "310\n",
      "(200, 10, 4) (200, 10)\n",
      "311\n",
      "(200, 10, 4) (200, 10)\n",
      "312\n",
      "(200, 10, 4) (200, 10)\n",
      "313\n",
      "(200, 10, 4) (200, 10)\n",
      "314\n",
      "(200, 10, 4) (200, 10)\n",
      "315\n",
      "(200, 10, 4) (200, 10)\n",
      "316\n",
      "(200, 10, 4) (200, 10)\n",
      "317\n",
      "(200, 10, 4) (200, 10)\n",
      "318\n",
      "(200, 10, 4) (200, 10)\n",
      "319\n",
      "(200, 10, 4) (200, 10)\n",
      "320\n",
      "(200, 10, 4) (200, 10)\n",
      "321\n",
      "(200, 10, 4) (200, 10)\n",
      "322\n",
      "(200, 10, 4) (200, 10)\n",
      "323\n",
      "(200, 10, 4) (200, 10)\n",
      "324\n",
      "(200, 10, 4) (200, 10)\n",
      "325\n",
      "(200, 10, 4) (200, 10)\n",
      "326\n",
      "(200, 10, 4) (200, 10)\n",
      "327\n",
      "(200, 10, 4) (200, 10)\n",
      "328\n",
      "(200, 10, 4) (200, 10)\n",
      "329\n",
      "(200, 10, 4) (200, 10)\n",
      "330\n",
      "(200, 10, 4) (200, 10)\n",
      "331\n",
      "(200, 10, 4) (200, 10)\n",
      "332\n",
      "(200, 10, 4) (200, 10)\n",
      "333\n",
      "(200, 10, 4) (200, 10)\n",
      "334\n",
      "(200, 10, 4) (200, 10)\n",
      "335\n",
      "(200, 10, 4) (200, 10)\n",
      "336\n",
      "(200, 10, 4) (200, 10)\n",
      "337\n",
      "(200, 10, 4) (200, 10)\n",
      "338\n",
      "(200, 10, 4) (200, 10)\n",
      "339\n",
      "(200, 10, 4) (200, 10)\n",
      "340\n",
      "(200, 10, 4) (200, 10)\n",
      "341\n",
      "(200, 10, 4) (200, 10)\n",
      "342\n",
      "(200, 10, 4) (200, 10)\n",
      "343\n",
      "(200, 10, 4) (200, 10)\n",
      "344\n",
      "(200, 10, 4) (200, 10)\n",
      "345\n",
      "(200, 10, 4) (200, 10)\n",
      "346\n",
      "(200, 10, 4) (200, 10)\n",
      "347\n",
      "(200, 10, 4) (200, 10)\n",
      "348\n",
      "(200, 10, 4) (200, 10)\n",
      "349\n",
      "(200, 10, 4) (200, 10)\n",
      "350\n",
      "(200, 10, 4) (200, 10)\n",
      "351\n",
      "(200, 10, 4) (200, 10)\n",
      "352\n",
      "(200, 10, 4) (200, 10)\n",
      "353\n",
      "(200, 10, 4) (200, 10)\n",
      "354\n",
      "(200, 10, 4) (200, 10)\n",
      "355\n",
      "(200, 10, 4) (200, 10)\n",
      "356\n",
      "(200, 10, 4) (200, 10)\n",
      "357\n",
      "(200, 10, 4) (200, 10)\n",
      "358\n",
      "(200, 10, 4) (200, 10)\n",
      "359\n",
      "(200, 10, 4) (200, 10)\n",
      "360\n",
      "(200, 10, 4) (200, 10)\n",
      "361\n",
      "(200, 10, 4) (200, 10)\n",
      "362\n",
      "(200, 10, 4) (200, 10)\n",
      "363\n",
      "(200, 10, 4) (200, 10)\n",
      "364\n",
      "(200, 10, 4) (200, 10)\n",
      "365\n",
      "(200, 10, 4) (200, 10)\n",
      "366\n",
      "(200, 10, 4) (200, 10)\n",
      "367\n",
      "(200, 10, 4) (200, 10)\n",
      "368\n",
      "(200, 10, 4) (200, 10)\n",
      "369\n",
      "(200, 10, 4) (200, 10)\n",
      "370\n",
      "(200, 10, 4) (200, 10)\n",
      "371\n",
      "(200, 10, 4) (200, 10)\n",
      "372\n",
      "(200, 10, 4) (200, 10)\n",
      "373\n",
      "(200, 10, 4) (200, 10)\n",
      "374\n",
      "(200, 10, 4) (200, 10)\n",
      "375\n",
      "(200, 10, 4) (200, 10)\n",
      "376\n",
      "(200, 10, 4) (200, 10)\n",
      "377\n",
      "(200, 10, 4) (200, 10)\n",
      "378\n",
      "(200, 10, 4) (200, 10)\n",
      "379\n",
      "(200, 10, 4) (200, 10)\n",
      "380\n",
      "(200, 10, 4) (200, 10)\n",
      "381\n",
      "(200, 10, 4) (200, 10)\n",
      "382\n",
      "(200, 10, 4) (200, 10)\n",
      "383\n",
      "(200, 10, 4) (200, 10)\n",
      "384\n",
      "(200, 10, 4) (200, 10)\n",
      "385\n",
      "(200, 10, 4) (200, 10)\n",
      "386\n",
      "(200, 10, 4) (200, 10)\n",
      "387\n",
      "(200, 10, 4) (200, 10)\n",
      "388\n",
      "(200, 10, 4) (200, 10)\n",
      "389\n",
      "(200, 10, 4) (200, 10)\n",
      "390\n",
      "(200, 10, 4) (200, 10)\n",
      "391\n",
      "(200, 10, 4) (200, 10)\n",
      "392\n",
      "(200, 10, 4) (200, 10)\n",
      "393\n",
      "(200, 10, 4) (200, 10)\n",
      "394\n",
      "(200, 10, 4) (200, 10)\n",
      "395\n",
      "(200, 10, 4) (200, 10)\n",
      "396\n",
      "(200, 10, 4) (200, 10)\n",
      "397\n",
      "(200, 10, 4) (200, 10)\n",
      "398\n",
      "(200, 10, 4) (200, 10)\n",
      "399\n",
      "(200, 10, 4) (200, 10)\n",
      "400\n",
      "(200, 10, 4) (200, 10)\n",
      "401\n",
      "(200, 10, 4) (200, 10)\n",
      "402\n",
      "(200, 10, 4) (200, 10)\n",
      "403\n",
      "(200, 10, 4) (200, 10)\n",
      "404\n",
      "(200, 10, 4) (200, 10)\n",
      "405\n",
      "(200, 10, 4) (200, 10)\n",
      "406\n",
      "(200, 10, 4) (200, 10)\n",
      "407\n",
      "(200, 10, 4) (200, 10)\n",
      "408\n",
      "(200, 10, 4) (200, 10)\n",
      "409\n",
      "(200, 10, 4) (200, 10)\n",
      "410\n",
      "(200, 10, 4) (200, 10)\n",
      "411\n",
      "(200, 10, 4) (200, 10)\n",
      "412\n",
      "(200, 10, 4) (200, 10)\n",
      "413\n",
      "(200, 10, 4) (200, 10)\n",
      "414\n",
      "(200, 10, 4) (200, 10)\n",
      "415\n",
      "(200, 10, 4) (200, 10)\n",
      "416\n",
      "(200, 10, 4) (200, 10)\n",
      "417\n",
      "(200, 10, 4) (200, 10)\n",
      "418\n",
      "(200, 10, 4) (200, 10)\n",
      "419\n",
      "(200, 10, 4) (200, 10)\n",
      "420\n",
      "(200, 10, 4) (200, 10)\n",
      "421\n",
      "(200, 10, 4) (200, 10)\n",
      "422\n",
      "(200, 10, 4) (200, 10)\n",
      "423\n",
      "(200, 10, 4) (200, 10)\n",
      "424\n",
      "(200, 10, 4) (200, 10)\n",
      "425\n",
      "(200, 10, 4) (200, 10)\n",
      "426\n",
      "(200, 10, 4) (200, 10)\n",
      "427\n",
      "(200, 10, 4) (200, 10)\n",
      "428\n",
      "(200, 10, 4) (200, 10)\n",
      "429\n",
      "(200, 10, 4) (200, 10)\n",
      "430\n",
      "(200, 10, 4) (200, 10)\n",
      "431\n",
      "(200, 10, 4) (200, 10)\n",
      "432\n",
      "(200, 10, 4) (200, 10)\n",
      "433\n",
      "(200, 10, 4) (200, 10)\n",
      "434\n",
      "(200, 10, 4) (200, 10)\n",
      "435\n",
      "(200, 10, 4) (200, 10)\n",
      "436\n",
      "(200, 10, 4) (200, 10)\n",
      "437\n",
      "(200, 10, 4) (200, 10)\n",
      "438\n",
      "(200, 10, 4) (200, 10)\n",
      "439\n",
      "(200, 10, 4) (200, 10)\n",
      "440\n",
      "(200, 10, 4) (200, 10)\n",
      "441\n",
      "(200, 10, 4) (200, 10)\n",
      "442\n",
      "(200, 10, 4) (200, 10)\n",
      "443\n",
      "(200, 10, 4) (200, 10)\n",
      "444\n",
      "(200, 10, 4) (200, 10)\n",
      "445\n",
      "(200, 10, 4) (200, 10)\n",
      "446\n",
      "(200, 10, 4) (200, 10)\n",
      "447\n",
      "(200, 10, 4) (200, 10)\n",
      "448\n",
      "(200, 10, 4) (200, 10)\n",
      "449\n",
      "(200, 10, 4) (200, 10)\n",
      "450\n",
      "(200, 10, 4) (200, 10)\n",
      "451\n",
      "(200, 10, 4) (200, 10)\n",
      "452\n",
      "(200, 10, 4) (200, 10)\n",
      "453\n",
      "(200, 10, 4) (200, 10)\n",
      "454\n",
      "(200, 10, 4) (200, 10)\n",
      "455\n",
      "(200, 10, 4) (200, 10)\n",
      "456\n",
      "(200, 10, 4) (200, 10)\n",
      "457\n",
      "(200, 10, 4) (200, 10)\n",
      "458\n",
      "(200, 10, 4) (200, 10)\n",
      "459\n",
      "(200, 10, 4) (200, 10)\n",
      "460\n",
      "(200, 10, 4) (200, 10)\n",
      "461\n",
      "(200, 10, 4) (200, 10)\n",
      "462\n",
      "(200, 10, 4) (200, 10)\n",
      "463\n",
      "(200, 10, 4) (200, 10)\n",
      "464\n",
      "(200, 10, 4) (200, 10)\n",
      "465\n",
      "(200, 10, 4) (200, 10)\n",
      "466\n",
      "(200, 10, 4) (200, 10)\n",
      "467\n",
      "(200, 10, 4) (200, 10)\n",
      "468\n",
      "(200, 10, 4) (200, 10)\n",
      "469\n",
      "(200, 10, 4) (200, 10)\n",
      "470\n",
      "(200, 10, 4) (200, 10)\n",
      "471\n",
      "(200, 10, 4) (200, 10)\n",
      "472\n",
      "(200, 10, 4) (200, 10)\n",
      "473\n",
      "(200, 10, 4) (200, 10)\n",
      "474\n",
      "(200, 10, 4) (200, 10)\n",
      "475\n",
      "(200, 10, 4) (200, 10)\n",
      "476\n",
      "(200, 10, 4) (200, 10)\n",
      "477\n",
      "(200, 10, 4) (200, 10)\n",
      "478\n",
      "(200, 10, 4) (200, 10)\n",
      "479\n",
      "(200, 10, 4) (200, 10)\n",
      "480\n",
      "(200, 10, 4) (200, 10)\n",
      "481\n",
      "(200, 10, 4) (200, 10)\n",
      "482\n",
      "(200, 10, 4) (200, 10)\n",
      "483\n",
      "(200, 10, 4) (200, 10)\n",
      "484\n",
      "(200, 10, 4) (200, 10)\n",
      "485\n",
      "(200, 10, 4) (200, 10)\n",
      "486\n",
      "(200, 10, 4) (200, 10)\n",
      "487\n",
      "(200, 10, 4) (200, 10)\n",
      "488\n",
      "(200, 10, 4) (200, 10)\n",
      "489\n",
      "(200, 10, 4) (200, 10)\n",
      "490\n",
      "(200, 10, 4) (200, 10)\n",
      "491\n",
      "(200, 10, 4) (200, 10)\n",
      "492\n",
      "(200, 10, 4) (200, 10)\n",
      "493\n",
      "(200, 10, 4) (200, 10)\n",
      "494\n",
      "(200, 10, 4) (200, 10)\n",
      "495\n",
      "(200, 10, 4) (200, 10)\n",
      "496\n",
      "(200, 10, 4) (200, 10)\n",
      "497\n",
      "(200, 10, 4) (200, 10)\n",
      "498\n",
      "(200, 10, 4) (200, 10)\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "    for step, (X, Y) in enumerate(epoch):\n",
    "        print (X.shape,Y.shape)\n",
    "        print (step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack_1:0\", shape=(200,), dtype=float32) Tensor(\"add_10:0\", shape=(200, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, [batch_size, num_steps,n_time_series], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_steps], name='labels_placeholder')\n",
    "    init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "    \"\"\"\n",
    "    RNN Inputs\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn our x placeholder into a list of one-hot tensors:\n",
    "    # rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "    rnn_inputs = tf.unstack(x, axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "    Definition of rnn_cell\n",
    "\n",
    "    This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L95\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('rnn_cell'):\n",
    "        W = tf.get_variable('W', [n_time_series + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    def rnn_cell(rnn_input, state):\n",
    "        with tf.variable_scope('rnn_cell', reuse=True):\n",
    "            W = tf.get_variable('W', [n_time_series + state_size, state_size])\n",
    "            b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Adding rnn_cells to graph\n",
    "\n",
    "    This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "    Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "    \"\"\"\n",
    "    state = init_state\n",
    "    rnn_outputs = []\n",
    "    for rnn_input in rnn_inputs:\n",
    "        state = rnn_cell(rnn_input, state)\n",
    "        rnn_outputs.append(state)\n",
    "    final_state = rnn_outputs[-1]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Predictions, loss, training step\n",
    "\n",
    "    Losses is similar to the \"sequence_loss\"\n",
    "    function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "    \"\"\"\n",
    "\n",
    "    #logits and predictions\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, 1])\n",
    "        b = tf.get_variable('b', [1], initializer=tf.constant_initializer(0.0))\n",
    "    predictions = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "    # Turn our y placeholder into a list of labels\n",
    "    y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "    print (y_as_list[0], predictions[0])\n",
    "    #losses and train_step\n",
    "    losses = [tf.nn.l2_loss(actual - prediction) for \\\n",
    "              prediction, actual in zip(predictions, y_as_list)]\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "[[0 2 8 ..., 2 0 2]\n",
      " [1 4 0 ..., 2 0 3]\n",
      " [7 7 9 ..., 6 6 7]\n",
      " [7 8 4 ..., 1 2 7]]\n",
      "[15 21 21 ..., 11  8 19]\n",
      "Average loss at step 100 for last 250 steps: 1036264.48438\n",
      "Average loss at step 200 for last 250 steps: 658809.459375\n",
      "Average loss at step 300 for last 250 steps: 659685.135625\n",
      "Average loss at step 400 for last 250 steps: 661198.238125\n",
      "\n",
      "EPOCH 1\n",
      "[[7 1 3 ..., 0 2 6]\n",
      " [0 5 2 ..., 8 3 5]\n",
      " [4 9 6 ..., 0 6 7]\n",
      " [9 5 2 ..., 8 5 9]]\n",
      "[20 20 13 ..., 16 16 27]\n",
      "Average loss at step 100 for last 250 steps: 667694.606875\n",
      "Average loss at step 200 for last 250 steps: 659888.958125\n",
      "Average loss at step 300 for last 250 steps: 664343.996875\n",
      "Average loss at step 400 for last 250 steps: 659175.534375\n",
      "\n",
      "EPOCH 2\n",
      "[[3 0 3 ..., 5 0 8]\n",
      " [0 6 1 ..., 0 1 9]\n",
      " [3 5 4 ..., 3 2 1]\n",
      " [4 8 8 ..., 9 7 6]]\n",
      "[10 19 16 ..., 17 10 24]\n",
      "Average loss at step 100 for last 250 steps: 664002.9225\n",
      "Average loss at step 200 for last 250 steps: 661801.290625\n",
      "Average loss at step 300 for last 250 steps: 660342.616875\n",
      "Average loss at step 400 for last 250 steps: 662644.59\n",
      "\n",
      "EPOCH 3\n",
      "[[2 3 0 ..., 6 2 0]\n",
      " [8 7 0 ..., 2 8 4]\n",
      " [2 3 4 ..., 5 7 5]\n",
      " [6 3 1 ..., 8 3 3]]\n",
      "[18 16  5 ..., 21 20 12]\n",
      "Average loss at step 100 for last 250 steps: 665912.03875\n",
      "Average loss at step 200 for last 250 steps: 662160.37625\n",
      "Average loss at step 300 for last 250 steps: 664797.34625\n",
      "Average loss at step 400 for last 250 steps: 658414.7575\n",
      "\n",
      "EPOCH 4\n",
      "[[2 5 3 ..., 9 5 2]\n",
      " [5 2 1 ..., 8 2 9]\n",
      " [7 0 4 ..., 3 5 5]\n",
      " [9 5 2 ..., 0 0 2]]\n",
      "[23 12 10 ..., 20 12 18]\n",
      "Average loss at step 100 for last 250 steps: 668235.133125\n",
      "Average loss at step 200 for last 250 steps: 660355.331875\n",
      "Average loss at step 300 for last 250 steps: 659742.794375\n",
      "Average loss at step 400 for last 250 steps: 657904.074375\n",
      "\n",
      "EPOCH 5\n",
      "[[8 2 1 ..., 1 9 1]\n",
      " [7 8 5 ..., 0 9 1]\n",
      " [3 6 0 ..., 6 4 1]\n",
      " [5 8 9 ..., 3 0 9]]\n",
      "[23 24 15 ..., 10 22 12]\n",
      "Average loss at step 100 for last 250 steps: 665883.389375\n",
      "Average loss at step 200 for last 250 steps: 658976.280625\n",
      "Average loss at step 300 for last 250 steps: 660301.638125\n",
      "Average loss at step 400 for last 250 steps: 662763.744375\n",
      "\n",
      "EPOCH 6\n",
      "[[3 8 5 ..., 5 6 0]\n",
      " [8 8 2 ..., 5 9 7]\n",
      " [2 5 8 ..., 9 9 2]\n",
      " [8 5 6 ..., 2 4 6]]\n",
      "[21 26 21 ..., 21 28 15]\n",
      "Average loss at step 100 for last 250 steps: 666513.6675\n",
      "Average loss at step 200 for last 250 steps: 659053.979375\n",
      "Average loss at step 300 for last 250 steps: 665247.79875\n",
      "Average loss at step 400 for last 250 steps: 657317.415625\n",
      "\n",
      "EPOCH 7\n",
      "[[4 2 0 ..., 9 2 9]\n",
      " [8 8 4 ..., 3 8 6]\n",
      " [9 3 6 ..., 0 5 3]\n",
      " [9 4 5 ..., 2 1 9]]\n",
      "[30 17 15 ..., 14 16 27]\n",
      "Average loss at step 100 for last 250 steps: 664177.695625\n",
      "Average loss at step 200 for last 250 steps: 662251.775625\n",
      "Average loss at step 300 for last 250 steps: 659625.193125\n",
      "Average loss at step 400 for last 250 steps: 660485.383125\n",
      "\n",
      "EPOCH 8\n",
      "[[2 4 0 ..., 5 2 7]\n",
      " [8 3 7 ..., 5 5 9]\n",
      " [2 7 0 ..., 6 1 0]\n",
      " [6 1 0 ..., 5 8 0]]\n",
      "[18 15  7 ..., 21 16 16]\n",
      "Average loss at step 100 for last 250 steps: 663580.665625\n",
      "Average loss at step 200 for last 250 steps: 658290.92875\n",
      "Average loss at step 300 for last 250 steps: 661164.41\n",
      "Average loss at step 400 for last 250 steps: 659975.76\n",
      "\n",
      "EPOCH 9\n",
      "[[7 3 0 ..., 7 2 3]\n",
      " [7 1 1 ..., 1 7 0]\n",
      " [6 4 5 ..., 4 5 0]\n",
      " [7 1 8 ..., 6 1 3]]\n",
      "[27  9 14 ..., 18 15  6]\n",
      "Average loss at step 100 for last 250 steps: 666836.6825\n",
      "Average loss at step 200 for last 250 steps: 660429.4575\n",
      "Average loss at step 300 for last 250 steps: 661334.816875\n",
      "Average loss at step 400 for last 250 steps: 662123.780625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x113b1aeb8>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wMfw5V+gwI6sAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1129a4b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=16, verbose=True):\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #instantiate a saver object\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "            save_path = saver.save(sess, 'binary_seq.ckpt')\n",
    "\n",
    "    return training_losses\n",
    "\n",
    "\n",
    "training_losses = train_network(10,num_steps, state_size = 16)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 4)\n",
      "(1, 10, 4) 0\n",
      "[10 22 29 ..., 21 24 25]\n",
      "[array([[ 17.29883003]], dtype=float32), array([[ 18.0242157]], dtype=float32), array([[ 18.02471924]], dtype=float32), array([[ 18.02365875]], dtype=float32), array([[ 18.02242088]], dtype=float32), array([[ 18.0228405]], dtype=float32), array([[ 18.02455139]], dtype=float32), array([[ 18.01909065]], dtype=float32), array([[ 18.02340126]], dtype=float32), array([[ 18.02129173]], dtype=float32)]\n",
      "(1, 10) 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 10) for Tensor 'Placeholder:0', which has shape '(1, 10, 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-51513687a284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mprimer\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_state\u001b[0m  \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/crytting/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/crytting/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    945\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 10) for Tensor 'Placeholder:0', which has shape '(1, 10, 4)'"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #add in a primer so we can generate a sequence\n",
    "    primer = tf.placeholder(tf.float32, [1, num_steps, n_time_series])\n",
    "    \n",
    "    init_state = tf.zeros([1, state_size])   \n",
    "\n",
    "    rnn_inputs = tf.unstack(primer,axis = 1)\n",
    "\n",
    "    with tf.variable_scope('rnn_cell'):\n",
    "        W = tf.get_variable('W', [n_time_series + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    def rnn_cell(rnn_input, state):\n",
    "        with tf.variable_scope('rnn_cell', reuse=True):\n",
    "            W = tf.get_variable('W', [n_time_series + state_size, state_size])\n",
    "            b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "    \n",
    "    #Throw the primer through the rnn_cell\n",
    "\n",
    "    \"\"\"\n",
    "    Adding rnn_cells to graph\n",
    "\n",
    "    This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "    Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "    \"\"\"\n",
    "    state = init_state\n",
    "    rnn_outputs = []\n",
    "    for rnn_input in rnn_inputs:\n",
    "        state = rnn_cell(rnn_input, state)\n",
    "        rnn_outputs.append(state)\n",
    "    final_state = rnn_outputs[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Predictions, loss, training step\n",
    "\n",
    "    Losses is similar to the \"sequence_loss\"\n",
    "    function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "    \"\"\"\n",
    "\n",
    "    #logits and predictions\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, 1])\n",
    "        b = tf.get_variable('b', [1], initializer=tf.constant_initializer(0.0))\n",
    "    predictions = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './binary_seq.ckpt')\n",
    "    training_state = np.zeros((1,state_size))\n",
    "    data = gen_data()\n",
    "    X, Y = data\n",
    "    primer_ = X.T[:num_steps,:]\n",
    "    primer_ = primer_[np.newaxis, :, :]\n",
    "    print (primer_.shape)\n",
    "    forecast = primer_\n",
    "    for i in range(20):\n",
    "        print (forecast.shape, i)\n",
    "        feed_dict = {primer : forecast, init_state: training_state  }\n",
    "        preds, training_state = session.run([predictions, final_state], feed_dict = feed_dict)\n",
    "        forecast = [int(np.argmax(prediction, axis = 1)) for prediction in preds]\n",
    "        forecast =  np.array(forecast)[np.newaxis, :]\n",
    "        print (Y)\n",
    "        print (preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
